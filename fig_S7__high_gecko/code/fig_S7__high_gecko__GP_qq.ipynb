{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import uncertainty_toolbox as uct\n",
    "\n",
    "# mpl.use(\"Cairo\")  # for saving SVGs that Affinity Designer can parse\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib as pl\n",
    "import dill\n",
    "\n",
    "import candas as can\n",
    "import gumbi as gmb\n",
    "from candas.learn import ParameterSet\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "code_pth = pl.Path.cwd()  # for running in Jupyter\n",
    "# code_pth = pl.Path(__file__)  # for running in terminal\n",
    "fig_pth = code_pth.parent\n",
    "data_pth = fig_pth / \"data\"\n",
    "graph_pth = fig_pth / \"graphics\"\n",
    "graph_pth.mkdir(exist_ok=True)\n",
    "\n",
    "gen_pth = fig_pth / \"generated\"\n",
    "gen_pth.mkdir(exist_ok=True)\n",
    "\n",
    "plt.style.use(str(can.style.breve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use('style.mplstyle')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# from utils import savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = ParameterSet.load(data_pth / \"ADVI_ParameterSets_220528.pkl\")\n",
    "\n",
    "\n",
    "def make_pair(row):\n",
    "    return \"-\".join(sorted([row.FPrimer, row.RPrimer]))\n",
    "\n",
    "\n",
    "data = (\n",
    "    ps.wide.query('Metric == \"mean\"')\n",
    "    .astype({\"BP\": float})\n",
    "    .assign(PrimerPair=lambda df: df.apply(make_pair, axis=1))\n",
    "    .groupby([\"Target\", \"PrimerPair\", \"Reporter\"])\n",
    "    .mean(numeric_only=True)\n",
    "    .drop_duplicates()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "selected = (\n",
    "    data.groupby([\"PrimerPair\", \"Reporter\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"Observations\"})\n",
    "    .sort_values(\"Observations\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ").iloc[[0, 1, 4, 5, 6, 8, 38, 39, 42]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_full = gmb.DataSet(\n",
    "    data=data,\n",
    "    outputs=[\"F0_lg\", \"r\", \"K\", \"m\"],\n",
    "    log_vars=[\"BP\", \"K\", \"m\", \"r\"],\n",
    "    logit_vars=[\"GC\"],\n",
    ")\n",
    "stdzr = ds_full.stdzr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data = data[\n",
    "            ~((data.PrimerPair == 'FP004-RP004') & (data.Reporter == 'HEX'))\n",
    "        ]\n",
    "\n",
    "xval_data = data[\n",
    "            (data.PrimerPair == 'FP004-RP004') & (data.Reporter == 'HEX')\n",
    "        ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 10\n",
    "i = 3\n",
    "I = len(xval_data)\n",
    "    \n",
    "def get_train_vec(x, i):\n",
    "    \"\"\"\n",
    "    Get a training vector for the cross-validation\n",
    "    \"\"\"\n",
    "    is_train = np.array([1]*i + [0]*(I-i)).astype(bool)\n",
    "    np.random.RandomState(x*I+i).shuffle(is_train)\n",
    "    return is_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2bbb3ec6ee4f4c9a705be207a60975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "rows = []\n",
    "# train_dss = []\n",
    "# train_μσts = []\n",
    "test_dss = []\n",
    "test_μσts = []\n",
    "models = []\n",
    "\n",
    "x = 0\n",
    "\n",
    "if (gen_pth / \"Ind_model_qq.csv\").exists():\n",
    "    df = pd.read_csv(gen_pth / \"Ind_model_qq.csv\")\n",
    "    print(df[[\"N_train\", \"Iteration\"]].astype(int).apply(tuple, axis=1).tolist())\n",
    "else:\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"N_train\", \"Iteration\", \"Train_pred_q\", \"Train_obsd_q\", \"TrainCode\"],\n",
    "    ).astype({\"N_train\": int, \"Iteration\": int})\n",
    "\n",
    "# for i in tqdm(range(1, I + 1, 5), desc=\"Training set size\", leave=True):\n",
    "i = 6\n",
    "for j in tqdm(range(J), desc=\"Iteration\", leave=False):\n",
    "\n",
    "    so_far = df[[\"N_train\", \"Iteration\"]].astype(int).apply(tuple, axis=1).tolist()\n",
    "    if (i, j) in so_far:\n",
    "        continue\n",
    "\n",
    "    is_train = get_train_vec(j, i)\n",
    "    train_code = \"\".join(is_train.astype(int).astype(str))\n",
    "    k = 0\n",
    "    while train_code in df[\"TrainCode\"].values:\n",
    "        is_train = get_train_vec(j * 1000 + k * I, i)\n",
    "        train_code = \"\".join(is_train.astype(int).astype(str))\n",
    "        k += 1\n",
    "\n",
    "    train_data = xval_data.iloc[is_train]\n",
    "    test_data = xval_data.iloc[~is_train]\n",
    "\n",
    "    train_ds = gmb.DataSet(\n",
    "        data=train_data,\n",
    "        outputs=[\"F0_lg\", \"r\", \"K\", \"m\"],\n",
    "        log_vars=[\"BP\", \"K\", \"m\", \"r\"],\n",
    "        logit_vars=[\"GC\"],\n",
    "        stdzr=stdzr,\n",
    "    )\n",
    "\n",
    "    test_ds = gmb.DataSet(\n",
    "        data=test_data,\n",
    "        outputs=[\"F0_lg\", \"r\", \"K\", \"m\"],\n",
    "        log_vars=[\"BP\", \"K\", \"m\", \"r\"],\n",
    "        logit_vars=[\"GC\"],\n",
    "        stdzr=stdzr,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        if train_ds.wide.shape[0] > 1:\n",
    "            gp = gmb.GP(train_ds).fit(continuous_dims=[\"BP\", \"GC\"], progressbar=False)\n",
    "        else:\n",
    "            gp = gmb.GP(train_ds)\n",
    "            gp.specify_model(continuous_dims=[\"BP\", \"GC\"])\n",
    "            gp.filter_dims = {}\n",
    "            gp.continuous_dims = [\"BP\", \"GC\"]\n",
    "            gp.continuous_levels = gp._parse_levels(gp.continuous_dims, None)\n",
    "            gp.continuous_coords = gp._parse_coordinates(\n",
    "                gp.continuous_dims, gp.continuous_levels, None\n",
    "            )\n",
    "            gp.build_model()\n",
    "            gp.find_MAP(progressbar=False)\n",
    "\n",
    "        test_preds = gp.predict_points(\n",
    "            gp.parray(**test_ds.wide[[\"BP\", \"GC\"]].to_dict(orient=\"list\"))\n",
    "        ).get(\"r\")\n",
    "        train_preds = gp.predict_points(\n",
    "            gp.parray(**train_ds.wide[[\"BP\", \"GC\"]].to_dict(orient=\"list\"))\n",
    "        ).get(\"r\")\n",
    "    except LinAlgError:\n",
    "        continue\n",
    "\n",
    "    models.append(gp)\n",
    "    # train_dss.append(train_ds)\n",
    "    # train_μσts.append((train_preds.μ, train_preds.σ, train_ds.wide.r.values))\n",
    "    test_dss.append(test_data)\n",
    "    μ, σ, t = (test_preds.μ, test_preds.σ, test_ds.wide.r.values)\n",
    "\n",
    "    test_μσts.append((μ, σ, t))\n",
    "\n",
    "    (exp_proportions, obs_proportions) = uct.get_proportion_lists(\n",
    "        μ, σ, t, prop_type=\"interval\"\n",
    "    )\n",
    "\n",
    "    n = len(exp_proportions)\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            pd.DataFrame(\n",
    "                data=np.array(\n",
    "                    [\n",
    "                        [i] * n,\n",
    "                        [j] * n,\n",
    "                        exp_proportions,\n",
    "                        obs_proportions,\n",
    "                        [train_code] * n,\n",
    "                    ]\n",
    "                ).T,\n",
    "                columns=[\"N_train\", \"Iteration\", \"Train_pred_q\", \"Train_obsd_q\", \"TrainCode\"],\n",
    "            ).astype({\"N_train\": int, \"Iteration\": int}),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df.to_csv(gen_pth / \"Ind_model_qq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab073d94fb7e4e50a8bfdc4b15e8b23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "rows = []\n",
    "# train_dss = []\n",
    "# train_μσts = []\n",
    "test_dss = []\n",
    "test_μσts = []\n",
    "models = []\n",
    "\n",
    "x = 0\n",
    "\n",
    "if (gen_pth / \"Avg_model_qq.csv\").exists():\n",
    "    df = pd.read_csv(gen_pth / \"Avg_model_qq.csv\")\n",
    "    print(df[[\"N_train\", \"Iteration\"]].astype(int).apply(tuple, axis=1).tolist())\n",
    "else:\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"N_train\", \"Iteration\", \"Train_pred_q\", \"Train_obsd_q\", \"TrainCode\"],\n",
    "    ).astype({\"N_train\": int, \"Iteration\": int})\n",
    "\n",
    "# for i in tqdm(range(1, I + 1, 5), desc=\"Training set size\", leave=True):\n",
    "i = 6\n",
    "for j in tqdm(range(J), desc=\"Iteration\", leave=False):\n",
    "\n",
    "    so_far = df[[\"N_train\", \"Iteration\"]].astype(int).apply(tuple, axis=1).tolist()\n",
    "    if (i, j) in so_far:\n",
    "        continue\n",
    "\n",
    "    is_train = get_train_vec(j, i)\n",
    "    train_code = \"\".join(is_train.astype(int).astype(str))\n",
    "    k = 0\n",
    "    while train_code in df[\"TrainCode\"].values:\n",
    "        is_train = get_train_vec(j * 1000 + k * I, i)\n",
    "        train_code = \"\".join(is_train.astype(int).astype(str))\n",
    "        k += 1\n",
    "\n",
    "    train_data = pd.concat([static_data, xval_data.iloc[is_train]])\n",
    "    test_data = xval_data.iloc[~is_train]\n",
    "\n",
    "    train_ds = gmb.DataSet(\n",
    "        data=train_data,\n",
    "        outputs=[\"F0_lg\", \"r\", \"K\", \"m\"],\n",
    "        log_vars=[\"BP\", \"K\", \"m\", \"r\"],\n",
    "        logit_vars=[\"GC\"],\n",
    "        stdzr=stdzr,\n",
    "    )\n",
    "\n",
    "    test_ds = gmb.DataSet(\n",
    "        data=test_data,\n",
    "        outputs=[\"F0_lg\", \"r\", \"K\", \"m\"],\n",
    "        log_vars=[\"BP\", \"K\", \"m\", \"r\"],\n",
    "        logit_vars=[\"GC\"],\n",
    "        stdzr=stdzr,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        gp = gmb.GP(train_ds).fit(\n",
    "            continuous_dims=[\"BP\", \"GC\"], progressbar=False\n",
    "        )\n",
    "        test_preds = gp.predict_points(\n",
    "            gp.parray(**test_ds.wide[[\"BP\", \"GC\"]].to_dict(orient=\"list\"))\n",
    "        ).get(\"r\")\n",
    "        train_preds = gp.predict_points(\n",
    "            gp.parray(**xval_data.iloc[is_train][[\"BP\", \"GC\"]].to_dict(orient=\"list\"))\n",
    "        ).get(\"r\")\n",
    "    except LinAlgError:\n",
    "        continue\n",
    "\n",
    "    models.append(gp)\n",
    "    # train_dss.append(train_ds)\n",
    "    # train_μσts.append((train_preds.μ, train_preds.σ, train_ds.wide.r.values))\n",
    "    test_dss.append(test_data)\n",
    "    μ, σ, t = (test_preds.μ, test_preds.σ, test_ds.wide.r.values)\n",
    "\n",
    "    test_μσts.append((μ, σ, t))\n",
    "\n",
    "    (exp_proportions, obs_proportions) = uct.get_proportion_lists(\n",
    "        μ, σ, t, prop_type=\"interval\"\n",
    "    )\n",
    "\n",
    "    n = len(exp_proportions)\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            pd.DataFrame(\n",
    "                data=np.array(\n",
    "                    [\n",
    "                        [i] * n,\n",
    "                        [j] * n,\n",
    "                        exp_proportions,\n",
    "                        obs_proportions,\n",
    "                        [train_code] * n,\n",
    "                    ]\n",
    "                ).T,\n",
    "                columns=[\"N_train\", \"Iteration\", \"Train_pred_q\", \"Train_obsd_q\", \"TrainCode\"],\n",
    "            ).astype({\"N_train\": int, \"Iteration\": int}),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df.to_csv(gen_pth / \"Avg_model_qq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 5), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 7), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 8), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9), (6, 9)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690b816a73de472cbdb5f53eb8652385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "rows = []\n",
    "# train_dss = []\n",
    "# train_μσts = []\n",
    "test_dss = []\n",
    "test_μσts = []\n",
    "models = []\n",
    "\n",
    "x = 0\n",
    "\n",
    "if (gen_pth / \"LMC_model_qq.csv\").exists():\n",
    "    df = pd.read_csv(gen_pth / \"LMC_model_qq.csv\")\n",
    "    print(df[[\"N_train\", \"Iteration\"]].astype(int).apply(tuple, axis=1).tolist())\n",
    "else:\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"N_train\", \"Iteration\", \"Train_pred_q\", \"Train_obsd_q\", \"TrainCode\"],\n",
    "    ).astype({\"N_train\": int, \"Iteration\": int})\n",
    "\n",
    "# for i in tqdm(range(1, I + 1, 5), desc=\"Training set size\", leave=True):\n",
    "i = 6\n",
    "for j in tqdm(range(J), desc=\"Iteration\", leave=False):\n",
    "\n",
    "    so_far = df[[\"N_train\", \"Iteration\"]].astype(int).apply(tuple, axis=1).tolist()\n",
    "    if (i, j) in so_far:\n",
    "        continue\n",
    "\n",
    "    is_train = get_train_vec(j, i)\n",
    "    train_code = \"\".join(is_train.astype(int).astype(str))\n",
    "    k = 0\n",
    "    while train_code in df[\"TrainCode\"].values:\n",
    "        is_train = get_train_vec(j * 1000 + k * I, i)\n",
    "        train_code = \"\".join(is_train.astype(int).astype(str))\n",
    "        k += 1\n",
    "\n",
    "    train_data = pd.concat([static_data, xval_data.iloc[is_train]])\n",
    "    test_data = xval_data.iloc[~is_train]\n",
    "\n",
    "    train_ds = gmb.DataSet(\n",
    "        data=train_data,\n",
    "        outputs=[\"F0_lg\", \"r\", \"K\", \"m\"],\n",
    "        log_vars=[\"BP\", \"K\", \"m\", \"r\"],\n",
    "        logit_vars=[\"GC\"],\n",
    "        stdzr=stdzr,\n",
    "    )\n",
    "\n",
    "    test_ds = gmb.DataSet(\n",
    "        data=test_data,\n",
    "        outputs=[\"F0_lg\", \"r\", \"K\", \"m\"],\n",
    "        log_vars=[\"BP\", \"K\", \"m\", \"r\"],\n",
    "        logit_vars=[\"GC\"],\n",
    "        stdzr=stdzr,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        gp = gmb.GP(train_ds).fit(\n",
    "            continuous_dims=[\"BP\", \"GC\"],\n",
    "            categorical_dims=[\"PrimerPair\", \"Reporter\"],\n",
    "            progressbar=False,\n",
    "        )\n",
    "\n",
    "        test_pa = gp.parray(**test_ds.wide[[\"BP\", \"GC\"]].to_dict(orient=\"list\"))\n",
    "        test_pa = gp.append_categorical_points(\n",
    "            test_pa, {\"PrimerPair\": \"FP004-RP004\", \"Reporter\": \"HEX\"}\n",
    "        )\n",
    "        test_preds = gp.predict_points(test_pa).get(\"r\")\n",
    "\n",
    "        train_pa = gp.parray(\n",
    "            **xval_data.iloc[is_train][[\"BP\", \"GC\"]].to_dict(orient=\"list\")\n",
    "        )\n",
    "        train_pa = gp.append_categorical_points(\n",
    "            train_pa, {\"PrimerPair\": \"FP004-RP004\", \"Reporter\": \"HEX\"}\n",
    "        )\n",
    "        train_preds = gp.predict_points(train_pa).get(\"r\")\n",
    "    except LinAlgError:\n",
    "        continue\n",
    "    \n",
    "    models.append(gp)\n",
    "    # train_dss.append(train_ds)\n",
    "    # train_μσts.append((train_preds.μ, train_preds.σ, train_ds.wide.r.values))\n",
    "    test_dss.append(test_data)\n",
    "    μ, σ, t = (test_preds.μ, test_preds.σ, test_ds.wide.r.values)\n",
    "\n",
    "    test_μσts.append((μ, σ, t))\n",
    "\n",
    "    (exp_proportions, obs_proportions) = uct.get_proportion_lists(\n",
    "        μ, σ, t, prop_type=\"interval\"\n",
    "    )\n",
    "\n",
    "    n = len(exp_proportions)\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            pd.DataFrame(\n",
    "                data=np.array(\n",
    "                    [\n",
    "                        [i] * n,\n",
    "                        [j] * n,\n",
    "                        exp_proportions,\n",
    "                        obs_proportions,\n",
    "                        [train_code] * n,\n",
    "                    ]\n",
    "                ).T,\n",
    "                columns=[\"N_train\", \"Iteration\", \"Train_pred_q\", \"Train_obsd_q\", \"TrainCode\"],\n",
    "            ).astype({\"N_train\": int, \"Iteration\": int}),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df.to_csv(gen_pth / \"LMC_model_qq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 0), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 1), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 2), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 3), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4), (6, 4)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a123552fa8e34e9b8402ea3e6e8d9eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "rows = []\n",
    "# train_dss = []\n",
    "# train_μσts = []\n",
    "test_dss = []\n",
    "test_μσts = []\n",
    "models = []\n",
    "\n",
    "x = 0\n",
    "\n",
    "if (gen_pth / \"LMC_model2_qq.csv\").exists():\n",
    "    df = pd.read_csv(gen_pth / \"LMC_model2_qq.csv\")\n",
    "    print(df[[\"N_train\", \"Iteration\"]].astype(int).apply(tuple, axis=1).tolist())\n",
    "else:\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"N_train\", \"Iteration\", \"Train_pred_q\", \"Train_obsd_q\", \"TrainCode\"],\n",
    "    ).astype({\"N_train\": int, \"Iteration\": int})\n",
    "\n",
    "# for i in tqdm(range(1, I + 1, 5), desc=\"Training set size\", leave=True):\n",
    "i = 6\n",
    "for j in tqdm(range(J), desc=\"Iteration\", leave=False):\n",
    "\n",
    "    so_far = df[[\"N_train\", \"Iteration\"]].astype(int).apply(tuple, axis=1).tolist()\n",
    "    if (i, j) in so_far:\n",
    "        continue\n",
    "\n",
    "    is_train = get_train_vec(j, i)\n",
    "    train_code = \"\".join(is_train.astype(int).astype(str))\n",
    "    k = 0\n",
    "    while train_code in df[\"TrainCode\"].values:\n",
    "        is_train = get_train_vec(j * 1000 + k * I, i)\n",
    "        train_code = \"\".join(is_train.astype(int).astype(str))\n",
    "        k += 1\n",
    "\n",
    "    train_data = pd.concat([static_data, xval_data.iloc[is_train]])\n",
    "    test_data = xval_data.iloc[~is_train]\n",
    "\n",
    "    train_ds = gmb.DataSet(\n",
    "        data=train_data,\n",
    "        outputs=[\"F0_lg\", \"r\", \"K\", \"m\"],\n",
    "        log_vars=[\"BP\", \"K\", \"m\", \"r\"],\n",
    "        logit_vars=[\"GC\"],\n",
    "        stdzr=stdzr,\n",
    "    )\n",
    "\n",
    "    test_ds = gmb.DataSet(\n",
    "        data=test_data,\n",
    "        outputs=[\"F0_lg\", \"r\", \"K\", \"m\"],\n",
    "        log_vars=[\"BP\", \"K\", \"m\", \"r\"],\n",
    "        logit_vars=[\"GC\"],\n",
    "        stdzr=stdzr,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        gp = gmb.GP(train_ds).fit(\n",
    "            continuous_dims=[\"BP\", \"GC\"],\n",
    "            categorical_dims=[\"PrimerPair\"],\n",
    "            progressbar=False,\n",
    "        )\n",
    "\n",
    "        test_pa = gp.parray(**test_ds.wide[[\"BP\", \"GC\"]].to_dict(orient=\"list\"))\n",
    "        test_pa = gp.append_categorical_points(\n",
    "            test_pa, {\"PrimerPair\": \"FP004-RP004\"}\n",
    "        )\n",
    "        test_preds = gp.predict_points(test_pa).get(\"r\")\n",
    "\n",
    "        train_pa = gp.parray(\n",
    "            **xval_data.iloc[is_train][[\"BP\", \"GC\"]].to_dict(orient=\"list\")\n",
    "        )\n",
    "        train_pa = gp.append_categorical_points(\n",
    "            train_pa, {\"PrimerPair\": \"FP004-RP004\"}\n",
    "        )\n",
    "        train_preds = gp.predict_points(train_pa).get(\"r\")\n",
    "    except LinAlgError:\n",
    "        continue\n",
    "    \n",
    "    models.append(gp)\n",
    "    # train_dss.append(train_ds)\n",
    "    # train_μσts.append((train_preds.μ, train_preds.σ, train_ds.wide.r.values))\n",
    "    test_dss.append(test_data)\n",
    "    μ, σ, t = (test_preds.μ, test_preds.σ, test_ds.wide.r.values)\n",
    "\n",
    "    test_μσts.append((μ, σ, t))\n",
    "\n",
    "    (exp_proportions, obs_proportions) = uct.get_proportion_lists(\n",
    "        μ, σ, t, prop_type=\"interval\"\n",
    "    )\n",
    "\n",
    "    n = len(exp_proportions)\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            pd.DataFrame(\n",
    "                data=np.array(\n",
    "                    [\n",
    "                        [i] * n,\n",
    "                        [j] * n,\n",
    "                        exp_proportions,\n",
    "                        obs_proportions,\n",
    "                        [train_code] * n,\n",
    "                    ]\n",
    "                ).T,\n",
    "                columns=[\"N_train\", \"Iteration\", \"Train_pred_q\", \"Train_obsd_q\", \"TrainCode\"],\n",
    "            ).astype({\"N_train\": int, \"Iteration\": int}),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df.to_csv(gen_pth / \"LMC_model2_qq.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py,ipynb"
  },
  "kernelspec": {
   "display_name": "can_manuscript",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
